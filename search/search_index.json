{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ADIT - Automated DICOM Transfer","text":"\u276e \u276f <p>ADIT acts as an intelligent bridge between traditional DICOM systems and modern applications, enabling secure, controlled, and privacy-preserving medical imaging data transfer.</p> <p>Traditional PACS and DICOM systems rely on specialized protocols that are not directly compatible with modern web applications. ADIT solves this challenge by acting as a translator between web-friendly APIs and native DICOM protocols\u2014without requiring changes to existing PACS configurations.</p>"},{"location":"#request-and-response-workflow","title":"Request and Response Workflow","text":"<p>ADIT simplifies interaction with DICOM systems through the following process:</p> <ul> <li> <p>You send a simple web request, similar to interacting with any standard REST API.</p> </li> <li> <p>ADIT translates the web request into traditional DICOM commands.</p> </li> <li> <p>ADIT communicates with the PACS using its native DICOM protocols.</p> </li> <li> <p>The PACS response is converted by ADIT into a web-friendly format.</p> </li> <li> <p>You receive easy-to-use JSON metadata or DICOM files.</p> </li> </ul>"},{"location":"#about","title":"About","text":""},{"location":"#developed-at","title":"Developed at","text":"<p>CCI Bonn - Center for Computational Imaging, University Hospital Bonn</p>"},{"location":"#partners","title":"Partners","text":"<ul> <li>Universit\u00e4tsklinikum Bonn</li> <li>Thoraxklinik Heidelberg</li> <li>Universit\u00e4tsklinikum Heidelberg</li> </ul> <p>Beta Status</p> <p>ADIT is currently in early beta stage. While we are actively building and refining its features, users should anticipate ongoing updates and potential breaking changes as the platform evolves. We appreciate your understanding and welcome feedback to help us shape the future of ADIT.</p> <p>Admin Guide: Explore system administration, configuration, and management features in the Admin Guide</p> <p>User Guide: Explore the application\u2019s features, and how to execute common workflows in a clear and practical manner in our User Guide</p>"},{"location":"Backups/","title":"Backups","text":"<p>For database backups django-dbbackup app is used. The backups are done every night at 3 am by a periodic task using the dbbackup management command and stored in the <code>backups</code> directory which is mounted as a volume. The dbbackup command can also be called manually with <code>uv run cli db-backup</code>.</p>"},{"location":"Maintenance/","title":"Maintenance","text":""},{"location":"Maintenance/#how-to-upgrade","title":"How to upgrade","text":"<p>There are different things that can be upgraded:</p> <ul> <li>The python package dependencies (normal dependencies and dev dependencies)</li> <li>Check outdated Python packages: <code>uv run cli show-outdated</code> (check Python section in output)</li> <li><code>uv lock --upgrade</code> will update packages according to their version range in <code>pyproject.toml</code></li> <li>Other upgrades (e.g. major versions) must be upgraded by modifying the version range in <code>pyproject.toml</code> before calling <code>uv lock --upgrade</code></li> <li>Javascript dependencies</li> <li>Check outdated Javascript packages: <code>uv run cli show-outdated</code> (check Javascript section in output)</li> <li><code>npm update</code> will update packages according to their version range in <code>package.json</code></li> <li>Other upgrades (e.g. major versions) must be upgraded by modifying the version range in <code>packages.json</code> before calling <code>npm update</code></li> <li>After an upgrade make sure the files in <code>static/vendor</code> still link to the correct files in <code>node_modules</code>1</li> <li>Python and uv in <code>Dockerfile</code> that builds the container where ADIT runs in</li> <li>Dependent services in <code>docker-compose.base.yml</code>, like PostgreSQL or Vespa database</li> <li>Github Codespaces development container dependencies in <code>.devcontainer/devcontainer.json</code> and <code>.devcontainer/Dockerfile</code></li> <li>Github actions <code>.github/workflows/ci.yml</code> dependencies</li> </ul>"},{"location":"dev-docs/architecture/","title":"ADIT Architecture Documentation","text":"<p>This document provides a comprehensive overview of ADIT's architecture, implementation details, and key components for developers.</p>"},{"location":"dev-docs/architecture/#system-overview","title":"System Overview","text":"<p>ADIT (Automated DICOM Transfer) is a full-stack web application designed for automated DICOM transfers. The system consists of a Django-based backend, PostgreSQL database, and server-side rendered web interface enhanced with HTMX for dynamic interactions.</p> <p>ADIT inherits common functionality from ADIT Radis Shared, a shared library that provides core components including user authentication, token-based authentication, common utilities, and shared Django applications used by both ADIT and RADIS projects.</p>"},{"location":"dev-docs/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<p>The ADIT platform provides automated DICOM retrieval, transformation, and transfer through coordinated Docker containers. Users access the system via web browser or ADIT Client (Python library for programmatic access), performing operations such as creating transfer jobs, uploading DICOM files, and configuring destinations.</p> <p>The system consists of three main components: a Django API server handling web UI and orchestration, a PostgreSQL database storing all persistent data and serving as the task queue, and transfer workers executing DICOM operations in the background.</p>"},{"location":"dev-docs/architecture/#backend-architecture","title":"Backend Architecture","text":"<p>Django Web/API Server: Central coordination engine providing REST API endpoints, authentication, user/session management, static assets, and task orchestration. Creates job/task records in PostgreSQL and schedules background work.</p> <p>PostgreSQL Database: System of record storing user accounts, transfer jobs, DICOM node configuration, task queue entries, execution history, and study metadata.</p> <p>Transfer Workers: Docker containers polling PostgreSQL for tasks, executing C-GET/C-MOVE/DICOMweb operations, applying pseudonymization, and logging results.</p>"},{"location":"dev-docs/architecture/#procrastinate-task-queue-system","title":"Procrastinate Task Queue System","text":"<p>ADIT uses Procrastinate, a PostgreSQL-based task queue storing jobs directly in the database without external message brokers. Tasks are Python functions with decorators, supporting job scheduling, prioritization, retry logic with exponential backoff, cancellation, and dead letter queues. Workers scale horizontally with configurable concurrency, graceful shutdown, and health monitoring.</p> <p>ADIT Task Types: <code>process_dicom_task</code> (core transfers), <code>check_disk_space</code>, <code>backup_db</code>. Features include configurable timeouts (20-minute process timeout), concurrency control via queue management, complete job history and logs in PostgreSQL, retry logic with exponential backoff (max 3 attempts), and graceful error handling.</p>"},{"location":"dev-docs/architecture/#orthanc-integration","title":"Orthanc Integration","text":"<p>ADIT uses Orthanc (open-source DICOM server) as a development and testing tool. Bundled Orthanc instances provide mock PACS environments for local development, automated testing, and protocol validation. Supports full DIMSE (C-FIND, C-MOVE, C-GET, C-STORE) and DICOMweb (WADO-RS, QIDO-RS, STOW-RS) protocols.</p>"},{"location":"dev-docs/architecture/#dicom-libraries","title":"DICOM Libraries","text":"<p>pydicom: Python library for reading, modifying, and writing DICOM files. ADIT uses it to work with DICOM datasets in memory (e.g., <code>from pydicom import Dataset</code>), parse DICOM tags, and convert DICOM data to other formats.</p> <p>pynetdicom: Python implementation of DICOM networking protocols. ADIT uses it to communicate with remote PACS servers over the network\u2014sending query requests (C-FIND), retrieving images (C-GET/C-MOVE), and accepting incoming DICOM transfers (C-STORE). It handles the low-level network communication while pydicom handles the DICOM file data.</p> <p>dicognito: DICOM anonymization library used for pseudonymization of patient data. ADIT leverages dicognito to remove identifying information from DICOM headers, replace patient names/IDs with pseudonyms, and maintain consistency across multiple studies for the same patient. Under the hood, dicognito uses pydicom.</p>"},{"location":"dev-docs/architecture/#frontend-architecture","title":"Frontend Architecture","text":"<p>Server-side rendered with Django templates and HTMX for dynamic interactions. Uses Bootstrap 5 for styling and Alpine.js for interactive components.</p> <p>ADIT Client: Python package (<code>adit-client</code>) for programmatic API access, supporting automated DICOM operations and returning pydicom datasets.</p>"},{"location":"dev-docs/architecture/#docker-container-architecture","title":"Docker Container Architecture","text":"<p>ADIT runs as multiple Docker containers that work together. In development, these containers run inside a VS Code dev container which provides a consistent development environment with Docker-in-Docker support, allowing you to run and manage the application containers from within the development container.</p>"},{"location":"dev-docs/architecture/#container-types","title":"Container Types","text":"<p>Web Container (<code>adit-web-1</code>): Runs Django application serving web UI and REST API. Ports: 8000 (dev), 80/443 (prod with SSL). Handles authentication, serves static files, enqueues tasks, and manages database connections.</p> <p>PostgreSQL Container (<code>adit-postgres-1</code>): PostgreSQL 17 database storing all data (users, jobs, tasks, logs, Procrastinate queue). Port 5432. Uses Docker volumes for persistence.</p> <p>Default Worker Container (<code>adit-default_worker-1</code>): Processes background tasks in the default queue (e.g., disk space checks, database backups).</p> <p>DICOM Worker Container (<code>adit-dicom_worker-1</code>): Executes DICOM transfer tasks from the dicom queue. Same base image as web container plus DICOM tools. Multiple instances can run for scaling.</p> <p>C-STORE Receiver Container (<code>adit-receiver-1</code>): Accepts incoming DICOM data from C-MOVE operations. Ports: 11112 (DICOM), 14638 (file transmit). Forwards data to workers via TCP.</p> <p>Orthanc Containers (<code>adit-orthanc1-1</code>, <code>adit-orthanc2-1</code>): Development PACS instances for testing. Official Orthanc image. Ports 7501/7502 (dev only). Uses SQLite for development.</p>"},{"location":"dev-docs/architecture/#application-architecture","title":"Application Architecture","text":""},{"location":"dev-docs/architecture/#core-django-apps-structure","title":"Core Django Apps Structure","text":""},{"location":"dev-docs/architecture/#core-app-aditcore","title":"Core App (<code>adit.core</code>)","text":"<ul> <li>Purpose: Foundation services and shared components</li> <li>Components: User management, DICOM node configuration, base models, utilities</li> <li>Key Features: Authentication, authorization, DICOM server management</li> </ul>"},{"location":"dev-docs/architecture/#transfer-apps","title":"Transfer Apps","text":"<ul> <li>Batch Transfer (<code>adit.batch_transfer</code>): Bulk data transfer operations</li> <li>Selective Transfer (<code>adit.selective_transfer</code>): Individual study transfers</li> </ul>"},{"location":"dev-docs/architecture/#exploration-discovery","title":"Exploration &amp; Discovery","text":"<ul> <li>DICOM Explorer (<code>adit.dicom_explorer</code>): Interactive DICOM data browsing</li> <li>Batch Query (<code>adit.batch_query</code>): Bulk DICOM server queries</li> </ul>"},{"location":"dev-docs/architecture/#upload-system-aditupload","title":"Upload System (<code>adit.upload</code>)","text":"<ul> <li>File Upload: Direct DICOM file upload to ADIT</li> </ul>"},{"location":"dev-docs/architecture/#primary-models","title":"Primary Models","text":""},{"location":"dev-docs/architecture/#user-management","title":"User Management","text":"<ul> <li> <p>Users &amp; Groups: Django authentication with group-based access. Each user has an active group that determines which reports they can access.</p> </li> <li> <p>Permissions: Fine-grained access control for features (extractions, subscriptions, urgent priority).</p> </li> </ul>"},{"location":"dev-docs/architecture/#transfer-operations","title":"Transfer Operations","text":"<p>Transfer Jobs define transfer operations with owner and status tracking, containing one or more Transfer Tasks that specify study/series/instance operations. Each task progresses through states: PENDING, IN_PROGRESS, SUCCESS, FAILURE, or CANCELED.</p>"},{"location":"dev-docs/architecture/#dicom-configuration","title":"DICOM Configuration","text":"<p>DICOM Servers represent remote PACS/Orthanc instances, with DICOM Nodes defining source and destination configurations. The system supports both DIMSE protocols (C-FIND, C-GET, C-MOVE, C-STORE) and DICOMweb REST APIs (QIDO-RS, WADO-RS, STOW-RS). Downloading data from a DICOM server can done by using a DIMSE operation or by using DICOMweb REST calls. When using DIMSE operations C-GET is prioritized over C-MOVE as a worker can fetch the DICOM data directly from the server. When downloading data using a C-MOVE operation, ADIT commands the source DICOM server to send the data to a C-STORE SCP server of ADIT running in a separate container (Receiver) that receives the DICOM data and sends it back to the worker using a TCP Socket connection (FileTransmitter).</p>"},{"location":"dev-docs/architecture/#related-projects","title":"Related Projects","text":"<ul> <li>ADIT Radis Shared: Common library providing authentication, utilities, and shared Django apps for ADIT and RADIS</li> </ul>"},{"location":"dev-docs/contributing/","title":"Contributing to Our Project","text":"<p>We're excited that you're interested in contributing to our project! This document outlines the guidelines for contributing to our codebase. We follow the Google Python Style Guide to maintain consistency and readability across our project.</p> <p>Code Style We adhere to the Google Python Style Guide.</p>"},{"location":"dev-docs/contributing/#getting-started","title":"Getting Started","text":"<p>This repository includes a Dev Container. The Dev Container is a Docker container that provides the development environment (VS Code, Git, Docker CLI, Node.js, Python tools). It uses Docker-in-Docker to run the application containers inside it. This ensures all developers have identical environments and can manage ADIT's multi-container setup seamlessly. If you open the project in VS Code after cloning, you should see a prompt:</p> <p>\u201cReopen in Dev Container\u201d</p> <p>Click it, and VS Code will automatically build and open the development environment.</p>"},{"location":"dev-docs/contributing/#installation","title":"Installation","text":"<pre><code>git clone https://github.com/openradx/adit.git\ncd adit\nuv sync  # installs Python dependencies into a virtual environment\ncp ./example.env ./.env  # copy environment template (adjust DJANGO_SECRET_KEY and TOKEN_AUTHENTICATION_SALT)\nuv run cli compose-up  # builds and starts Docker containers\n</code></pre> <p>The development server will start at http://localhost:8000.</p> <p>Initial setup: The <code>compose-up</code> command automatically runs migrations, creates example users/groups, and populates test Orthanc instances with sample data.</p> <p>File watching: Code changes auto-reload the server. Dependency changes (pyproject.toml) trigger container rebuilds.</p>"},{"location":"dev-docs/contributing/#updating-your-development-environment","title":"Updating Your Development Environment","text":"<p>Pull latest changes:</p> <pre><code>git pull origin main\nuv sync  # update dependencies\nuv run cli compose-up  # restart containers (migrations run automatically)\n</code></pre> <p>After pulling changes:</p> <ul> <li>Migrations run automatically on container startup</li> <li>If containers don't start, rebuild: <code>uv run cli compose-build &amp;&amp; uv run cli compose-up</code></li> <li>For major database schema changes, consider backing up first: <code>uv run cli db-backup</code></li> </ul> <p>Dependency updates:</p> <ul> <li>Python packages are updated via <code>uv sync</code> when pyproject.toml changes</li> <li>Docker images update via <code>uv run cli compose-pull</code> (base images)</li> </ul> <p>The development server of the example project will be started on http://localhost:8000</p> <p>File changes will be automatically detected and the servers will be restarted. When library dependencies are changed, the containers will automatically be rebuilt and restarted.</p>"},{"location":"dev-docs/contributing/#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter bugs or have feature requests, please open an issue on GitHub. Include as much detail as possible, including steps to reproduce the issue.</p>"},{"location":"dev-docs/contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Fork the repository and create a new branch for your feature or bug fix.</li> <li>Make your changes and ensure that they adhere to the Google Python Style Guide.</li> <li>Write tests for your changes and ensure that all tests pass.</li> <li>Commit your changes to a new branch with a clear and descriptive commit message.</li> <li>Push your changes to your forked repository and create a pull request against the main repository.</li> <li>Ensure that your pull request is linked to an issue in the main repository.</li> </ol>"},{"location":"dev-docs/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the AGPL-3.0 license.</p>"},{"location":"dev-docs/core_components/","title":"Core Architecture &amp; Component Hierarchy in ADIT","text":"<p>This document outlines the hierarchical architecture of the ADIT system, focusing on how core components are shared and reused across different applications (Selective Transfer, Batch Transfer, and Batch Query).</p>"},{"location":"dev-docs/core_components/#overview","title":"Overview","text":"<p>The ADIT system follows a hierarchical architecture with the Core App (<code>adit.core</code>) providing foundational services and shared components. The core includes user management, DICOM node configuration, base models, utilities, and the critical <code>DicomOperator</code> for all DICOM operations.</p>"},{"location":"dev-docs/core_components/#core-component-hierarchy","title":"Core Component Hierarchy","text":""},{"location":"dev-docs/core_components/#1-model-inheritance-structure","title":"1. Model Inheritance Structure","text":"<p>The core provides base models that establish a consistent pattern across all DICOM applications:</p> <pre><code>classDiagram\n    class DicomJob {\n        +status: Status\n        +owner: User\n        +message: str\n        +created: DateTime\n        +start: DateTime\n        +end: DateTime\n        +queue_pending_tasks()\n        +reset_tasks()\n        +post_process()\n    }\n\n    class TransferJob {\n        +trial_protocol_id: str\n        +trial_protocol_name: str\n        +archive_password: str\n    }\n\n    class DicomTask {\n        +job: DicomJob\n        +source: DicomNode\n        +status: Status\n        +message: str\n        +log: str\n        +attempts: int\n        +start: DateTime\n        +end: DateTime\n    }\n\n    class TransferTask {\n        +destination: DicomNode\n        +patient_id: str\n        +study_uid: str\n        +series_uids: list\n        +pseudonym: str\n    }\n\n    DicomJob \"1\" *-- \"many\" DicomTask : contains\n    DicomJob &lt;|-- TransferJob\n    DicomJob &lt;|-- BatchQueryJob\n    TransferJob &lt;|-- SelectiveTransferJob\n    TransferJob &lt;|-- BatchTransferJob\n\n    DicomTask &lt;|-- TransferTask\n    DicomTask &lt;|-- BatchQueryTask\n    TransferTask &lt;|-- SelectiveTransferTask\n    TransferTask &lt;|-- BatchTransferTask\n\n    class SelectiveTransferJob {\n        +get_absolute_url()\n    }\n\n    class BatchTransferJob {\n        +project_name: str\n        +project_description: str\n        +ethics_application_id: str\n    }\n\n    class BatchQueryJob {\n        +project_name: str\n        +project_description: str\n    }</code></pre> <p>Key Benefits:</p> <ul> <li>Unified Job Management: All apps use the same job lifecycle (pending \u2192 in-progress \u2192 success/failure)</li> <li>Consistent Task Processing: Shared status tracking, retry logic, and error handling</li> <li>Common Admin Interface: Single admin interface for all job types through inheritance</li> </ul>"},{"location":"dev-docs/core_components/#2-dicomoperator-the-core-dicom-communication-hub","title":"2. DicomOperator - The Core DICOM Communication Hub","text":"<p>The <code>DicomOperator</code> class is the central abstraction for all DICOM operations, providing a unified interface regardless of the underlying protocol (DIMSE, DICOMweb).</p>"},{"location":"dev-docs/core_components/#dicomoperator-core-methods","title":"DicomOperator Core Methods","text":"<p>The DicomOperator provides several key methods that are reused across all ADIT applications:</p> <p>Query Operations - Used by all applications:</p> <ul> <li><code>find_patients()</code> - Search for patients matching criteria</li> <li><code>find_studies()</code> - Search for studies matching criteria (most frequently used)</li> <li><code>find_series()</code> - Search for series within studies</li> <li><code>find_images()</code> - Search for individual images</li> </ul> <p>Retrieval Operations - Used by transfer applications:</p> <ul> <li><code>fetch_study()</code> - Download all images in a study (extensively reused)</li> <li><code>fetch_series()</code> - Download all images in a specific series</li> <li><code>fetch_image()</code> - Download a single image</li> </ul> <p>Movement Operations - Used by transfer applications:</p> <ul> <li><code>move_study()</code> - Move a study between DICOM servers</li> <li><code>upload_images()</code> - Upload images to a destination server</li> </ul>"},{"location":"dev-docs/core_components/#how-find_studies-works","title":"How find_studies() Works","text":"<p>The <code>find_studies()</code> method is the most commonly used query method across all applications. It searches for medical imaging studies on a DICOM server based on criteria like patient name, date range, or modality.</p> <p>Protocol Selection: The method intelligently chooses the communication protocol based on what the target server supports:</p> <ol> <li>DIMSE C-FIND (traditional protocol) - Used if the server supports patient-root or study-root queries</li> <li>DICOMweb QIDO-RS (modern web-based protocol) - Used as fallback if DIMSE is unavailable</li> <li>Returns results progressively as an iterator, allowing applications to process studies as they're found</li> </ol>"},{"location":"dev-docs/core_components/#how-fetch_study-works","title":"How fetch_study() Works","text":"<p>The <code>fetch_study()</code> method is extensively reused in transfer applications to download complete studies from DICOM servers. It retrieves all DICOM images belonging to a specific study.</p> <p>Protocol Selection Priority: The method tries protocols in order of efficiency:</p> <ol> <li>WADO-RS (DICOMweb retrieval) - Preferred for modern servers, most efficient</li> <li>C-GET (DIMSE retrieval) - Traditional protocol, widely supported</li> <li>C-MOVE (DIMSE movement) - Legacy protocol for older systems</li> </ol> <p>Callback Processing: As each image is retrieved, it's immediately passed to a callback function. This allows applications to:</p> <ul> <li>Process images in real-time without waiting for the entire study</li> <li>Modify DICOM tags on-the-fly (pseudonymization, etc.)</li> <li>Save images directly to disk as they arrive</li> <li>Provide progress updates to users</li> </ul>"},{"location":"dev-docs/core_components/#3-processor-architecture","title":"3. Processor Architecture","text":"<p>All DICOM processing follows a consistent pattern through processor inheritance:</p> <pre><code>classDiagram\n    class DicomTaskProcessor {\n        &lt;&lt;abstract&gt;&gt;\n        +app_name: str\n        +dicom_task_class: type\n        +app_settings_class: type\n        +logs: list\n        +process()* ProcessingResult\n        +is_suspended() bool\n    }\n\n    class TransferTaskProcessor {\n        +source_operator: DicomOperator\n        +dest_operator: DicomOperator\n        +transfer_task: TransferTask\n        +_download_study()\n        +_find_study() ResultDataset\n        +_transfer_to_server()\n        +_transfer_to_folder()\n        +_transfer_to_archive()\n        +_transfer_to_nifti()\n    }\n\n    class BatchQueryTaskProcessor {\n        +query_task: BatchQueryTask\n        +operator: DicomOperator\n        +_find_patients() list\n        +_find_studies() list\n        +_query_studies() list\n        +_query_series() list\n    }\n\n    DicomTaskProcessor &lt;|-- TransferTaskProcessor\n    DicomTaskProcessor &lt;|-- BatchQueryTaskProcessor\n    TransferTaskProcessor &lt;|-- SelectiveTransferTaskProcessor\n    TransferTaskProcessor &lt;|-- BatchTransferTaskProcessor</code></pre> <p>Processor Responsibilities:</p> <ul> <li>DicomTaskProcessor: Base processing logic, suspension handling, logging</li> <li>TransferTaskProcessor: Study download, transfer logic, DICOM manipulation</li> <li>BatchQueryTaskProcessor: Large-scale DICOM queries, result aggregation</li> </ul>"},{"location":"dev-docs/core_components/#application-specific-usage-patterns","title":"Application-Specific Usage Patterns","text":""},{"location":"dev-docs/core_components/#selective-transfer-real-time-interactive-queries","title":"Selective Transfer - Real-time Interactive Queries","text":"<p>Core Components Used:</p> <ul> <li><code>DicomOperator.find_studies()</code> - Real-time study searching</li> <li><code>DicomOperator.fetch_study()</code> - Study download for transfers</li> <li>WebSocket infrastructure for real-time UI updates</li> </ul>"},{"location":"dev-docs/core_components/#websocket-architecture","title":"WebSocket Architecture","text":"<p>The WebSocket infrastructure enables real-time communication between the browser and server, allowing for interactive DICOM query operations with immediate feedback. This is primarily used by the Selective Transfer application to provide a responsive user experience during DICOM server queries.</p> <p>The WebSocket connection allows bidirectional, persistent communication between the client (browser) and server. Unlike traditional HTTP requests, WebSockets maintain an open connection, enabling the server to send updates to the client as soon as data becomes available.</p> <p>Execution pathway:</p> <ol> <li>Authentication Check: Verifies the user is logged in before allowing WebSocket connection</li> <li>Resource Initialization:</li> <li><code>query_operators</code>: List to track active DICOM operations for potential cancellation</li> <li><code>current_message_id</code>: Counter to handle message ordering and prevent race conditions</li> <li><code>pool</code>: ThreadPoolExecutor for running blocking DICOM operations in background threads</li> <li>Connection Acceptance: Establishes the WebSocket connection with the client</li> </ol> <p>Execution flow:</p> <ol> <li>Create DicomOperator: Instantiates a new DicomOperator for the specific DICOM server</li> <li>Track Operation: Adds operator to the list for potential cancellation</li> <li>Execute Query: Calls <code>query_studies()</code> which uses <code>DicomOperator.find_studies()</code> under the hood</li> <li>Progressive Streaming: As each study is found:</li> <li>Adds it to the results list</li> <li>Checks if this query is still current (not cancelled)</li> <li>Immediately sends updated results to the browser via WebSocket</li> </ol> <p>User searches for studies containing \"MRI\" and PatientName \"Smith\":</p> <ol> <li>Browser \u2192 Sends JSON: <code>{\"action\": \"query\", \"patient_name\": \"Smith\", \"modality\": \"MR\"}</code></li> <li>WebSocket Consumer \u2192 Creates DicomOperator, increments message_id to 123</li> <li>DicomOperator \u2192 Queries DICOM server using C-FIND or QIDO-RS</li> <li>First study found \u2192 WebSocket sends: <code>{\"studies\": [study1], \"message_id\": 123}</code></li> <li>Browser \u2192 Updates UI immediately with study1</li> <li>Second study found \u2192 WebSocket sends: <code>{\"studies\": [study1, study2], \"message_id\": 123}</code></li> <li>Browser \u2192 Updates UI with both studies</li> <li>All studies found \u2192 WebSocket sends: <code>{\"studies\": [study1, study2, study3], \"complete\": true}</code></li> </ol> <p>If user starts new search before completion:</p> <ul> <li>message_id increments to 124</li> <li>Old query (123) results are ignored</li> <li>New query (124) results are displayed immediately</li> </ul>"},{"location":"dev-docs/core_components/#batch-transfer-bulk-background-processing","title":"Batch Transfer - Bulk Background Processing","text":"<p>Core Components Used:</p> <ul> <li><code>DicomOperator.find_studies()</code> - Study validation</li> <li><code>DicomOperator.fetch_study()</code> - Bulk study downloads</li> <li><code>TransferTaskProcessor</code> - Background processing</li> </ul> <p>Bulk Operations:</p> <ul> <li>Processes multiple studies per job</li> <li>Background task queue processing</li> <li>CSV file parsing for bulk study definitions</li> <li>Progress tracking across multiple tasks</li> </ul>"},{"location":"dev-docs/core_components/#batch-query-large-scale-dicom-queries","title":"Batch Query - Large-scale DICOM Queries","text":"<p>Core Components Used:</p> <ul> <li><code>DicomOperator.find_patients()</code> - Patient discovery</li> <li><code>DicomOperator.find_studies()</code> - Study enumeration</li> <li><code>DicomOperator.find_series()</code> - Series-level queries</li> <li>Custom result aggregation</li> </ul>"},{"location":"dev-docs/core_components/#shared-utility-components","title":"Shared Utility Components","text":""},{"location":"dev-docs/core_components/#dicomoperator-factory-pattern","title":"DicomOperator Factory Pattern","text":"<p>The DicomOperator is instantiated on-demand by each application component when DICOM communication is needed. Rather than maintaining long-lived operator instances, applications create operators as needed by passing a DicomServer configuration object to the constructor.</p> <p>Usage across applications:</p> <ul> <li>Selective Transfer: WebSocket consumers create operators for each real-time query session</li> <li>Batch Transfer: Task processors create separate source and destination operators for each transfer task</li> <li>Batch Query: Task processors create a single operator instance to query the configured source server</li> </ul> <p>This factory pattern ensures each operation has a properly configured operator without coupling applications to specific operator instances or lifecycle management.</p>"},{"location":"dev-docs/core_components/#common-query-patterns","title":"Common Query Patterns","text":"<p>All applications construct DICOM queries using a consistent QueryDataset interface that abstracts DICOM query parameters. The query creation follows DICOM's hierarchical model with fields like PatientID and StudyInstanceUID combined with a QueryRetrieveLevel specification.</p> <p>Standard study-level queries include the patient identifier, study instance UID (when known), and explicitly set the retrieve level to \"STUDY\". This pattern is used universally across selective transfer, batch transfer, and batch query applications. The DicomOperator's <code>find_studies()</code> method processes these standardized queries and returns matching studies as iterable results.</p> <p>This consistency ensures predictable behavior and allows operators to optimize queries based on the retrieve level and available search parameters.</p>"},{"location":"dev-docs/core_components/#callback-based-data-processing","title":"Callback-based Data Processing","text":"<p>All fetch operations (study downloads, series retrieval) use a callback-driven architecture for processing DICOM datasets as they stream from the source server. Rather than downloading complete studies before processing, applications provide callback functions that receive each DICOM image dataset immediately upon arrival.</p> <p>Callback responsibilities include real-time dataset modification (such as pseudonymization by changing DICOM tags), saving images to disk as they arrive, and providing incremental progress updates to users. This streaming approach is used by all transfer operations to enable efficient memory usage and responsive progress tracking.</p> <p>The callback pattern allows applications to implement custom processing logic without modifying the core retrieval mechanisms in DicomOperator.</p>"},{"location":"dev-docs/core_components/#development-guidelines","title":"Development Guidelines","text":""},{"location":"dev-docs/core_components/#when-to-use-each-component","title":"When to Use Each Component","text":"<p>Use DicomOperator when:</p> <ul> <li>Performing any DICOM server communication</li> <li>Need protocol abstraction (DIMSE vs DICOMweb)</li> <li>Implementing query/retrieve/move operations</li> </ul> <p>Use TransferTaskProcessor when:</p> <ul> <li>Building new transfer-based applications</li> <li>Need study download/upload capabilities</li> <li>Require DICOM manipulation during transfer</li> </ul> <p>Use DicomTaskProcessor when:</p> <ul> <li>Building new DICOM applications</li> <li>Need background task processing</li> <li>Require job/task lifecycle management</li> </ul> <p>Use WebSocket Consumer when:</p> <ul> <li>Need real-time user interaction</li> <li>Progressive result updates required</li> <li>Interactive query/transfer workflows</li> </ul>"},{"location":"dev-docs/core_components/#extension-points","title":"Extension Points","text":"<p>Adding New Transfer Apps:</p> <ol> <li>Inherit from <code>TransferJob</code> and <code>TransferTask</code> models</li> <li>Create processor inheriting from <code>TransferTaskProcessor</code></li> <li>Leverage existing <code>DicomOperator.fetch_study()</code> functionality</li> </ol> <p>Adding New Query Apps:</p> <ol> <li>Inherit from <code>DicomJob</code> and <code>DicomTask</code> models</li> <li>Create processor inheriting from <code>DicomTaskProcessor</code></li> <li>Use appropriate <code>DicomOperator.find_*()</code> methods</li> </ol> <p>Adding Real-time Features:</p> <ol> <li>Create WebSocket consumer similar to <code>SelectiveTransferConsumer</code></li> <li>Use <code>DicomOperator</code> for DICOM operations</li> <li>Implement progressive result streaming</li> </ol> <p>This architecture ensures that each application leverges the core components effectively while maintaining clean separation of concerns and maximum code reuse.</p>"},{"location":"user-docs/admin-guide/","title":"Admin Guide","text":"<p>The Admin Guide is intended for system administrators and technical staff responsible for configuring, and maintaining ADIT for DICOM data transfer.</p>"},{"location":"user-docs/admin-guide/#installation","title":"Installation","text":"<pre><code>Clone the repository: git clone https://github.com/openradx/adit.git\ncd adit\nuv sync\ncp ./example.env ./.env  # copy example environment to .env\nuv run cli stack-deploy  # builds and starts Docker containers\n</code></pre>"},{"location":"user-docs/admin-guide/#updating-adit","title":"Updating ADIT","text":"<p>Follow these steps to safely update your ADIT:</p> <ol> <li>Verify no active jobs: Navigate to Django Admin \u2192 Jobs Overview and confirm nothing is running</li> <li>Enable maintenance mode: In Django Admin, navigate to Common \u2192 Project Settings and check the \"Maintenance mode\" checkbox, then save</li> <li>Navigate to Production folder</li> <li>Backup database: Run <code>uv run cli db-backup</code> to create a database backup</li> <li>Remove stack: Run <code>uv run cli stack-rm</code> to remove all Docker containers and services</li> <li>Pull latest changes: Run <code>git pull origin main</code> to fetch the latest code updates</li> <li>Update environment: Compare <code>example.env</code> with your <code>.env</code> file and add any new environment variables or update changed values</li> <li>Pull Docker images: Run <code>uv run cli compose-pull</code> to download the latest Docker images</li> <li>Deploy stack: Run <code>uv run cli stack-deploy</code> to rebuild and start all services with the updated code</li> <li>Disable maintenance mode: In Django Admin, navigate to Common \u2192 Project Settings and uncheck the \"Maintenance mode\" checkbox, then save</li> </ol>"},{"location":"user-docs/admin-guide/#user-and-group-management","title":"User and Group Management","text":"<p>Administrators can create users by navigating to the Django Admin section. Alternatively, users can self-register, after which an administrator must approve and activate their account.</p> <p>ADIT uses a group-based permission system:</p> <ul> <li>Groups define access to specific DICOM servers through source/destination permissions</li> <li>Users are assigned to one or more groups to inherit their permissions</li> </ul>"},{"location":"user-docs/admin-guide/#creating-and-managing-groups","title":"Creating and Managing Groups","text":"<ol> <li> <p>Access Django Admin:</p> </li> <li> <p>Log in as a staff user</p> </li> <li> <p>Go to Admin Section \u2192 Django Admin (available at <code>/django-admin/</code> URL path)</p> </li> <li> <p>Create/Edit Groups:</p> </li> <li> <p>Navigate to Authentication and Authorization \u2192 Groups</p> </li> <li>Click \"Add Group\" or edit an existing group</li> <li> <p>Give the group a Name (e.g., \"Radiologists\", \"Research Team\")</p> </li> <li> <p>Assign Permissions:</p> </li> <li> <p>In the group form, you'll see Available permissions and Chosen permissions</p> </li> <li>Select the permissions you want from the available list:<ul> <li><code>selective_transfer | selective transfer job | Can process urgently</code></li> <li><code>selective_transfer | selective transfer job | Can transfer unpseudonymized</code></li> <li><code>batch_transfer | batch transfer job | Can process urgently</code></li> <li><code>batch_transfer | batch transfer job | Can transfer unpseudonymized</code></li> <li>Plus other ADIT-specific permissions for viewing/adding jobs</li> </ul> </li> <li> <p>Move them to Chosen permissions</p> </li> <li> <p>Add Users to Group:</p> </li> <li> <p>In the Users section, select users from Available users</p> </li> <li>Move them to Chosen users</li> <li>Click Save to apply all changes</li> </ol>"},{"location":"user-docs/admin-guide/#server-and-folder-management","title":"Server and Folder Management","text":""},{"location":"user-docs/admin-guide/#server-management","title":"Server Management","text":"<p>To add or configure DICOM servers, use the Django Admin interface:</p> <ol> <li>Log in as an administrator</li> <li>Go to Admin Section \u2192 Django Admin (available at <code>/django-admin/</code> URL path)</li> <li>Navigate to Core \u2192 Dicom servers</li> <li>Click Add Dicom server</li> <li>Configure the server details:</li> </ol> <p>Basic Settings:</p> <ul> <li>Name: Friendly name for the server</li> <li>Ae title: DICOM Application Entity title</li> <li>Host: Server hostname or IP address</li> <li>Port: DICOM port number</li> </ul> <p>DICOM Protocol Support:</p> <ul> <li>Patient root find support: Enable C-FIND at patient root level</li> <li>Patient root get support: Enable C-GET at patient root level</li> <li>Patient root move support: Enable C-MOVE at patient root level</li> <li>Study root find support: Enable C-FIND at study root level</li> <li>Study root get support: Enable C-GET at study root level</li> <li>Study root move support: Enable C-MOVE at study root level</li> <li>Store scp support: Enable C-STORE SCP operations</li> </ul> <p>DICOMweb Settings (if applicable):</p> <ul> <li>Dicomweb root url: Base URL for DICOMweb services</li> <li>Dicomweb qido support: Enable QIDO-RS (queries)</li> <li>Dicomweb wado support: Enable WADO-RS (retrieval)</li> <li>Dicomweb stow support: Enable STOW-RS (storage)</li> <li>Dicomweb qido prefix: URL prefix for QIDO-RS endpoints</li> <li>Dicomweb wado prefix: URL prefix for WADO-RS endpoints</li> <li>Dicomweb stow prefix: URL prefix for STOW-RS endpoints</li> <li> <p>Dicomweb authorization header: Authentication header for DICOMweb requests</p> </li> <li> <p>Configure Group Access: In the DICOM node group accesses section, specify which groups can use this server as source or destination</p> </li> </ul> <p>DICOM Protocol Support</p> <p>To determine which DICOM protocols are supported by a server, consult the server's DICOM Conformance Statement.</p>"},{"location":"user-docs/admin-guide/#folder-management","title":"Folder Management","text":"<p>Administrators can configure upload folders and storage locations for DICOM files:</p> <ol> <li>Access Django Admin: Navigate to Django Admin \u2192 Admin Section</li> <li>Configure Folders: Go to Core \u2192 DICOM Folders</li> <li>Add or Edit Folder:</li> <li>Click Add Folder to create a new folder configuration</li> <li>Enter a Name for the folder (e.g., \"Research Uploads\", \"Clinical Archive\")</li> <li>Specify the Path where DICOM files should be stored</li> <li>Set the Quota: Define the disk quota for this folder in GB</li> <li>Configure When to inform admin: Set the threshold (as a percentage or absolute value) at which administrators should be notified about quota usage</li> <li>Assign to Groups: Link folders to groups to control which users can access specific storage locations</li> <li>Save: Click Save to apply changes</li> </ol> <p>Quota Monitoring</p> <p>Administrators will receive notifications when folder usage reaches the configured threshold, allowing proactive storage management.</p>"},{"location":"user-docs/admin-guide/#job-overview","title":"Job Overview","text":"<p>The Admin section includes a Job Overview section where administrators can:</p> <ul> <li>Monitor real-time job status across all transfer operations</li> <li>View jobs by status: Pending, In Progress, Completed, Failed, or Cancelled</li> <li>Track job history</li> </ul> <p>To access the Job Overview:</p> <ol> <li>Navigate to Admin Section \u2192 Job Overview</li> <li>Click on individual jobs for detailed information</li> </ol>"},{"location":"user-docs/admin-guide/#broadcasting-messages","title":"Broadcasting Messages","text":"<p>Administrators can send broadcast emails to all users:</p> <ol> <li>Navigate to Django Admin \u2192 Admin Section</li> <li>Look for the broadcast or messaging feature</li> <li>Compose your message and send to all users</li> </ol>"},{"location":"user-docs/admin-guide/#system-announcements","title":"System Announcements","text":"<p>System administrators can inform users about important updates, maintenance schedules, or system changes through the announcement feature.</p>"},{"location":"user-docs/admin-guide/#creating-announcements","title":"Creating Announcements","text":"<ol> <li>Access Admin Interface: Navigate to the Django admin interface (typically accessible at <code>/admin/</code>)</li> <li>Find Project Settings: Go to the \"Common\" section and select \"Project settings\"</li> <li>Edit Announcement: In the Project Settings form, locate the \"Announcement\" field</li> <li>Enter Message: Type your announcement message. HTML formatting is supported for rich text display</li> <li>Save Changes: Click \"Save\" to publish the announcement</li> </ol>"},{"location":"user-docs/admin-guide/#announcement-display","title":"Announcement Display","text":"<ul> <li>Announcements appear prominently on the main/home page</li> <li>All logged-in users will see the announcement when they access ADIT</li> </ul>"},{"location":"user-docs/admin-guide/#example-announcements","title":"Example Announcements","text":"<p>Maintenance Notice:</p> <pre><code>&lt;strong&gt;Scheduled Maintenance:&lt;/strong&gt; ADIT will be offline for maintenance on\n&lt;strong&gt;March 15, 2024 from 2:00 AM to 4:00 AM UTC&lt;/strong&gt;. Please plan your\ntransfers accordingly.\n</code></pre>"},{"location":"user-docs/admin-guide/#adit-client","title":"ADIT Client","text":"<p>ADIT client could be used to access all the features of ADIT without using the web interface.</p> <p>Basic Usage:</p> <pre><code>from adit_client import AditClient\n\n# Initialize client\nclient = AditClient(base_url=\"https://your-adit-server.com\", token=\"your-api-token\")\n\n# Search for studies\nstudies = client.search_studies(patient_id=\"12345\")\n\n# Transfer studies\nclient.transfer_study(study_uid=\"1.2.3.4.5\", destination=\"TARGET_AE\")\n</code></pre> <p>To create an API token for programmatic access:</p> <ol> <li>Log in to ADIT with your user account</li> <li>Navigate to Adit Administration:</li> <li>Look for Token Authentication in the main navigation</li> <li> <p>Click Add token</p> </li> <li> <p>Fill in the token details:</p> </li> <li> <p>Owner: Select the user who will own the token</p> </li> <li> <p>Token hashed: Paste the hashed token value (this is not auto-generated)</p> </li> <li> <p>Fraction: Specify the token\u2019s fraction</p> </li> <li> <p>Description: Add an optional description to identify the token\u2019s purpose</p> </li> <li> <p>Expires: Set the expiration date and time, or leave empty if the token should not expire</p> </li> <li> <p>Last used: Usually left blank; this is updated automatically by the system</p> </li> <li> <p>Save the token:</p> </li> <li> <p>Click Save, Save and add another, or Save and continue editing</p> </li> </ol>"},{"location":"user-docs/faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>This section addresses common questions</p> <p>Q: How does pseudonymization work in ADIT?</p> <p>A: ADIT uses the dicognito library to:</p> <ul> <li>Remove identifying information from DICOM headers</li> <li>Replace patient names/IDs with provided pseudonyms</li> <li>Add trial information if specified</li> <li>Maintain consistency across multiple studies for the same patient</li> </ul> <p>Q: What DICOM tags are anonymized?</p> <p>A: ADIT anonymizes standard identifying tags including:</p> <ul> <li>Patient Name, Patient ID, Patient Birth Date</li> <li>Referring Physician, Institution Name</li> <li>Other tags according to DICOM anonymization profiles</li> </ul> <p>The following date/time tags are preserved to maintain clinical context:</p> <ul> <li>Study Date, Study Time</li> <li>Series Date, Series Time</li> <li>Acquisition Date/Time</li> <li>Content Date/Time</li> </ul> <p>Q: Does ADIT preserve patient age during anonymization?</p> <p>A: Yes. While PatientBirthDate is anonymized (shifted by a random offset), ADIT uses the dicognito library which preserves the patient's age relative to the study date. This means an 80-year-old patient will still appear as 80 years old in the anonymized data, not as a 20-year-old. This is important for maintaining clinically relevant information while protecting patient identity.</p> <p>Q: Which DICOM protocols are supported for my server?</p> <p>To determine which DICOM protocols are supported by a server, consult the server's DICOM Conformance Statement.</p>"},{"location":"user-docs/features/","title":"Features","text":"<p>ADIT provides a comprehensive set of features for DICOM data management and transfer.</p> <ul> <li>Transfer DICOM data between DICOM-compatible servers</li> <li>Download DICOM data to a specified folder</li> <li>Pseudonymize DICOM data on the fly</li> <li>Specify a trial name for the transferred data (stored in the DICOM header)</li> <li>Easy web interface to select which studies to transfer or download</li> <li>Upload a batch file to make multiple queries on a DICOM server</li> <li>Upload a batch file to transfer or download multiple studies</li> <li>A REST API and API client to manage transfers programmatically by an external script (see below)</li> <li>Define when transfers should happen (for example, more workers at night to reduce server load on a PACS)</li> <li>Fine-grained control of what users can or can't do and what they can access</li> <li>Help modals with detailed information for the most important features</li> <li>An upload portal to upload DICOM images through a web interface that can be pseudonymized on the client (before the transfer happens)</li> </ul>"},{"location":"user-docs/features/#feature-requests","title":"\ud83d\udca1 Feature Requests","text":"<p>Have an idea for a new feature? Please submit a feature request on the GitHub repository.</p>"},{"location":"user-docs/technical-overview/","title":"Technical Overview: Bridging DICOM and Web Technologies","text":""},{"location":"user-docs/technical-overview/#the-challenge-traditional-dicom-vs-modern-web-workflows","title":"The Challenge: Traditional DICOM vs Modern Web Workflows","text":"<p>Many existing PACS servers, while robust, rely on older, specialized DICOM protocols (DIMSE) and often have web-based access (like DICOMweb) either not implemented or explicitly turned off for security reasons. This creates a significant hurdle for modern applications, especially those built for the web or requiring automated, scriptable access.</p> <p>ADIT (Automated DICOM Transfer) acts as an intelligent intermediary, a \"translator\" or \"proxy,\" that allows you to interact with your medical imaging data using familiar web technologies, even if the underlying PACS does not natively support them.</p>"},{"location":"user-docs/technical-overview/#dicom-protocol-overview","title":"DICOM Protocol Overview","text":""},{"location":"user-docs/technical-overview/#traditional-dicom-protocols-dimse","title":"Traditional DICOM Protocols (DIMSE)","text":"<p>Traditional DICOM communication relies on DIMSE (DICOM Message Service Element) services:</p> <ul> <li>C-FIND (Query): Search for studies, series, or instances based on criteria</li> <li>C-GET (Retrieve - Pull): Client requests and \"pulls\" images directly</li> <li>C-MOVE (Retrieve - Push): Instructs server to \"push\" images to another destination</li> <li>C-STORE (Store): Send DICOM images to a server</li> </ul> <p>These protocols operate over dedicated TCP/IP connections and require specialized DICOM toolkits.</p>"},{"location":"user-docs/technical-overview/#dicomweb-protocols-restful-apis","title":"DICOMweb Protocols (RESTful APIs)","text":"<p>DICOMweb standardizes web-based access to DICOM data using RESTful principles:</p> <pre><code>flowchart LR\n    A[Web Client] --&gt;|HTTP GET| B[DICOMweb Server]\n    B --&gt;|JSON/DICOM| A\n    A --&gt;|HTTP POST| B\n\n    subgraph Services[\"DICOMweb Services\"]\n        C[QIDO-RS&lt;br/&gt;Query]\n        D[WADO-RS&lt;br/&gt;Retrieve]\n        E[STOW-RS&lt;br/&gt;Store]\n    end\n\n    B -.-&gt; Services\n\n</code></pre> <ul> <li>QIDO-RS: Query using HTTP GET with URL parameters</li> <li>WADO-RS: Retrieve using HTTP GET requests</li> <li>STOW-RS: Store using HTTP POST requests</li> </ul>"},{"location":"user-docs/technical-overview/#how-adit-bridges-the-gap","title":"How ADIT Bridges the Gap","text":"<p>ADIT acts as a translation layer between modern web APIs and traditional DICOM protocols:</p> <pre><code>sequenceDiagram\n    participant Client as Your Script/App\n    participant ADIT as ADIT Server\n    participant Worker as ADIT Worker\n    participant PACS as PACS Server\n\n    Client-&gt;&gt;ADIT: HTTP GET /dicomweb/studies?PatientAge=020-030&amp;Modality=CT\n    Note over ADIT: Receives DICOMweb/REST request\n\n    ADIT-&gt;&gt;Worker: Internal translation\n    Note over Worker: Converts REST \u2192 DIMSE\n\n    Worker-&gt;&gt;PACS: C-FIND (DIMSE Protocol)\n    PACS--&gt;&gt;Worker: DICOM Response\n\n    Worker-&gt;&gt;ADIT: Internal processing\n    Note over ADIT: Converts DIMSE \u2192 REST\n\n    ADIT--&gt;&gt;Client: HTTP 200 + JSON Response</code></pre>"},{"location":"user-docs/technical-overview/#the-translation-process","title":"The Translation Process","text":"<ol> <li>Receiving Web Requests: ADIT receives standard HTTP/HTTPS DICOMweb requests</li> <li>Internal Translation: Converts DICOMweb/REST requests into DIMSE commands</li> <li>PACS Communication: Communicates with PACS using native DIMSE protocols</li> <li>Response Translation: Converts DICOM data back to web-friendly format</li> <li>Web Response: Returns JSON/DICOM data over HTTP/HTTPS</li> </ol>"},{"location":"user-docs/technical-overview/#security-benefits","title":"Security Benefits","text":"<p>ADIT addresses common security concerns:</p>"},{"location":"user-docs/technical-overview/#centralized-security-model","title":"Centralized Security Model","text":"<ul> <li>Single Point of Control: Secure ADIT instead of exposing multiple PACS endpoints</li> <li>Authentication &amp; Authorization: Implement fine-grained access controls</li> <li>Network Isolation: PACS can remain on internal networks</li> </ul>"},{"location":"user-docs/user-guide/","title":"User Guide","text":"<p>The User Guide is designed for end users who interact with ADIT to perform DICOM data transfers. It explains how to use the application\u2019s features, and execute common workflows in a clear and practical manner</p>"},{"location":"user-docs/user-guide/#functionalities-overview","title":"Functionalities Overview","text":"<p>When you log into ADIT, you'll see the home page with several sections:</p> <ul> <li>Selective Transfers: Search and select specific studies to transfer or download.</li> <li>Batch Query: Search for studies on a PACS server by using a batch file.</li> <li>Batch Transfer: Transfer or download multiple studies specified in a batch file.</li> <li>DICOM Explorer: Explore the DICOM data of a PACS server</li> <li>DICOM Upload: Upload DICOM files from your local system to a PACS server</li> </ul>"},{"location":"user-docs/user-guide/#1-selective-transfer","title":"1. Selective Transfer","text":"<p>To transfer a single DICOM study:</p> <ol> <li>Navigate to the \"Selective Transfer\" section</li> <li>Select your source DICOM server</li> <li>Enter search criteria (Patient ID, Study Date, etc.)</li> <li>Browse and select the study you want to transfer</li> <li>Choose your destination server</li> <li>Configure transfer options (pseudonymization, trial name, etc.)</li> <li>Start the transfer</li> </ol>"},{"location":"user-docs/user-guide/#2-batch-query","title":"2. Batch Query","text":"<p>With a Batch Query you can create a job to find data of multiple studies in a source DICOM / PACS server. Batch query jobs are put into a queue and will be processed by a worker when the time is right. You will get an Email when the job is finished (or failed for some reason).</p> <p>Each batch query job contains several query tasks that define what studies to search for. The search terms must be specified in an Excel file (.xlsx). The first row of the Excel file must contain the header with the column titles (see below). Each of the following rows represent a query task.</p> <p>Excel Data Format</p> <p>If PatientID or AccessionNumber contains leading zeros those are relevant as it is not a number but a text identifier. So make sure that your Excel file does not remove those leading zeros by setting the column type to text or add a single quote <code>'</code> as prefix to the text cell itself.</p> <p>These are the columns in the batch file to execute your queries:</p> <ul> <li>PatientID: The unique ID of the patient in the PACS.</li> <li>PatientName: The name of the patient.</li> <li>PatientBirthDate: The birth date of the patient.</li> <li>AccessionNumber: The Accession Number (a unique ID) of the study.</li> <li>From: Only include studies newer than or equal to this date.</li> <li>Until: Only include studies older than or equal to this date.</li> <li>Modality: The modality of the study. Multiple modalities to query can be provided as a comma separated list.</li> <li>SeriesDescription: Only include series of the study, whose series description match a certain case insensitive regular expression pattern (see introduction into using a regular expression and testing your regular expression).</li> <li>SeriesNumber: Only include series of the study with the specified series number. Multiple series numbers can be provided as a comma separated list.</li> <li>Pseudonym: A pseudonym to pseudonymize the images during a subsequent transfer with Batch Transfer.</li> </ul> <p>The patient must be identifiable by either \"PatientID\" or \"PatientName\" together with \"PatientBirthDate\". The remaining fields are optional and may limit the results for what you really need.</p>"},{"location":"user-docs/user-guide/#3-batch-transfer","title":"3. Batch Transfer","text":"<p>With this form you can create a new batch transfer job to transfer studies from a source server to a destination. Batch transfer jobs are put into a queue and will be processed by a worker when the time is right. You will get an Email when the job is finished (or failed for some reason).</p> <p>Each batch transfer job contains several transfer tasks that define what studies to transfer. This data must be specified in an Excel file (.xlsx). The first row of the Excel file must contain the header with the column titles. The following rows contain the data that identifies the studies to transfer.</p> <p>The required PatientID and StudyInstanceUID can be fetched by doing a \"Batch Query\". The resulting file of a batch query can be used for the batch transfer. So a batch query is usually a preparation step for a batch transfer.</p> <p>Excel Data Format</p> <p>If PatientID or AccessionNumber contains leading zeros those are relevant as it is not a number but a text identifier. So make sure that your Excel file does not remove those leading zeros by setting the column type to text or add a single quote <code>'</code> as prefix to the text cell itself.</p> <p>The following columns must be defined in the batch file:</p> <ul> <li>PatientID: The unique ID of the patient in the PACS. This column is required.</li> <li>StudyInstanceUID: A unique ID that identifies the study. This column is required.</li> <li>SeriesInstanceUID: An unique ID that identifies the series. This column is optional to only transfer specific series of a study.</li> <li>Pseudonym: A pseudonym to pseudonymize the images during transfer. This field is required if you don't have the permission to transfer unpseudonymized (the default).</li> </ul> <p>The \"SeriesInstanceUID\" is optional. If provided, only the specified series of the study will be transferred. The provided pseudonym is optional if you have the permissions to transfer unpseudonymized. It will be set as PatientID and PatientName. So it is recommended to use cryptic identifier strings (e.g. \"XFE3TEW2N\").</p>"},{"location":"user-docs/user-guide/#4-download-studies","title":"4. Download Studies","text":"<p>To download DICOM studies to a local folder:</p> <ol> <li>Search for the desired studies</li> <li>Select \"Download\" instead of \"Transfer\"</li> <li>Choose the download location</li> <li>Start the download process</li> </ol>"},{"location":"user-docs/user-guide/#5-upload-dicom-files","title":"5. Upload DICOM Files","text":"<p>To upload DICOM files to a PACS server:</p> <ol> <li>Navigate to the \"DICOM Upload\" section</li> <li>Select your destination DICOM server</li> <li>Choose the DICOM files or folders to upload</li> <li>Start the upload process</li> <li>Monitor the upload progress and verify completion</li> </ol>"},{"location":"user-docs/user-guide/#6-explore-dicom-data","title":"6. Explore DICOM Data","text":"<p>To browse and explore DICOM data on a server:</p> <ol> <li>Go to the \"DICOM Explorer\" section</li> <li>Select the DICOM server to explore</li> <li>Use the hierarchical navigation (Patient \u2192 Study \u2192 Series)</li> <li>View DICOM metadata and image information</li> </ol>"},{"location":"user-docs/user-guide/#7-adit-client-programmatic-access","title":"7. ADIT Client (Programmatic Access)","text":"<p>The ADIT Client is a Python package (<code>adit-client</code>) that provides programmatic API access to ADIT functionality. It enables automated DICOM operations through Python scripts and returns data as pydicom datasets for seamless integration into your workflows.</p> <p>Key Features:</p> <ul> <li>Automate repetitive DICOM transfer tasks</li> <li>Integrate ADIT operations into existing Python applications</li> <li>Retrieve DICOM data as pydicom datasets for analysis</li> <li>Execute batch operations programmatically</li> <li>Access all ADIT features without using the web interface</li> </ul> <p>When to Use:</p> <ul> <li>Automating regular transfer workflows</li> <li>Integrating DICOM transfers into data pipelines</li> <li>Processing large batches of studies programmatically</li> </ul>"}]}